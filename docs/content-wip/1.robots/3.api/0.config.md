# Config

### `enabled`

- Type: `boolean`
- Default: `true`
- Required: `false`

Conditionally toggle the module.


### `sitemap`

- Type: `string | string[] | false`
- Default: `false`

The sitemap URL(s) for the site. If you have multiple sitemaps, you can provide an array of URLs.

You must either define the runtime config `siteUrl` or provide the sitemap as absolute URLs.

```ts
export default defineNuxtConfig({
  robots: {
    sitemap: [
      '/sitemap-one.xml',
      '/sitemap-two.xml',
    ],
  },
})
```

### `allow`

- Type: `string[]`
- Default: `[]`
- Required: `false`

Allow paths to be indexed for the `*` user-agent (all robots).

### `disallow`

- Type: `string[]`
- Default: `[]`
- Required: `false`

Disallow paths from being indexed for the `*` user-agent (all robots).

### `groups`

- Type: `{ userAgent: []; allow: []; disallow: []; comments: [] }[]`
- Default: `[]`
- Required: `false`

Define more granular rules for the robots.txt. Each group is a set of rules for specific user agent(s).

```ts
export default defineNuxtConfig({
  robots: {
    groups: [
      {
        userAgents: ['AdsBot-Google-Mobile', 'AdsBot-Google-Mobile-Apps'],
        disallow: ['/admin'],
        allow: ['/admin/login'],
        comments: 'Allow Google AdsBot to index the login page but no-admin pages'
      },
    ]
  }
})
```

### `robotsEnabledValue`

- Type: `string`
- Default: `'index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1'`
- Required: `false`

The value to use when the site is indexable.

### `robotsDisabledValue`

- Type: `string`
- Default: `'noindex, nofollow'`
- Required: `false`

The value to use when the site is not indexable.

### `disallowNonIndexableRoutes`

- Type: `boolean`
- Default: `'false'`

Should route rules which disallow indexing be added to the `/robots.txt` file.

### `mergeWithRobotsTxtPath`

- Type: `boolean | string`
- Default: `true`
- Required: `false`

Specify a robots.txt path to merge the config from, relative to the root directory.

When set to `true`, the default path of `<publicDir>/robots.txt` will be used.

When set to `false`, no merging will occur.

### `blockNonSeoBots`

- Type: `boolean`
- Default: `false`
- Required: `false`

Blocks some non-SEO bots from crawling your site. This is not a replacement for a full-blown bot management solution, but it can help to reduce the load on your server.

See [const.ts](https://github.com/harlan-zw/nuxt-simple-robots/blob/main/src/const.ts#L6) for the list of bots that are blocked.

```ts
export default defineNuxtConfig({
  robots: {
    blockNonSeoBots: true
  }
})
```

### `debug`

- Type: `boolean`
- Default: `false`
- Required: `false`

Enables debug logs and a debug endpoint.


### `credits`

- Type: `boolean`
- Default: `true`
- Required: `false`

Control the module credit comment in the generated robots.txt file.

```txt
# START nuxt-simple-robots (indexable) <- credits
 ...
# END nuxt-simple-robots <- credits
```

```ts
export default defineNuxtConfig({
  robots: {
    credits: false
  }
})
```


### `siteUrl` - DEPRECATED

- Type: `string`

Used to ensure sitemaps are absolute URLs.

Note: This is only required when prerendering your site.

This is now handled by the [nuxt-site-config](https://github.com/harlan-zw/nuxt-site-config) module.

You should provide `url` through site config instead, otherwise see the module for more examples.

```ts
export default defineNuxtConfig({
  site: {
    url: process.env.NUXT_SITE_URL || 'https://example.com',
  },
})
```

### `indexable` - DEPRECATED

- Type: `boolean`
- Default: `process.env.NODE_ENV === 'production'`

Whether the site is indexable by search engines.

This is now handled by the [nuxt-site-config](https://github.com/harlan-zw/nuxt-site-config) module.

If you need to change the default,
then you should provide `indexable` through site config instead or see the module for more examples.

## Nuxt Hooks

### `robots:config`

**Type:** `async (config: ModuleOptions) => void | Promise<void>`

This hook allows you to modify the robots config before it is used to generate the robots.txt and meta tags.

```ts
export default defineNuxtConfig({
  hooks: {
    'robots:config': (config) => {
      // modify the config
      config.sitemap = '/sitemap.xml'
    },
  },
})
```
